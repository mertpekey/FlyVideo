{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/mpekey/Desktop/FlyVideo/Peak_Signal_Data'\n",
    "\n",
    "FEATURES = ['pose.prob_x','pose.prob_y','pose.halt_x','pose.halt_y',\n",
    "            'pose.thor_post_x','pose.thor_post_y','distance.origin-halt',\n",
    "            'distance.origin-prob','distance.origin-thor_post','distance.head-prob',\n",
    "            'distance.thor_post-halt','distance.avg(thor_post-joint1,thor_post-joint2,thor_post-joint3)',\n",
    "            'distance.avg(origin-joint1,origin-joint2,origin-joint3)']\n",
    "\n",
    "experiment_features = ['pose.prob_x', 'pose.prob_y', 'distance.head-prob', 'distance.origin-prob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(DATA_PATH, 'bouts_dict.pkl')\n",
    "with open(filename, 'rb') as f:\n",
    "    bouts_dict = pickle.load(f)\n",
    "\n",
    "true_peak_fn = os.path.join(DATA_PATH, 'true_peak_annotations.npy')\n",
    "true_peak_df_fn = os.path.join(DATA_PATH, 'true_annotations.pkl')\n",
    "\n",
    "true_peak_annotations_array = np.load(true_peak_fn)\n",
    "with open(true_peak_df_fn, 'rb') as f:\n",
    "    true_peak_annotations_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_data(fly, experiment, features):\n",
    "    \n",
    "    input_data = bouts_dict[fly][features[0]][experiment].reshape(-1,1)\n",
    "\n",
    "    for i in range(1, len(features)):\n",
    "        input_data = np.concatenate((input_data,\n",
    "                                    bouts_dict[fly][features[i]][experiment].reshape(-1,1)),\n",
    "                                    axis=1)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def calculate_metrics(predictions, labels):\n",
    "    TP = sum(p == 1 and l == 1 for p, l in zip(predictions, labels))\n",
    "    TN = sum(p == 0 and l == 0 for p, l in zip(predictions, labels))\n",
    "    FP = sum(p == 1 and l == 0 for p, l in zip(predictions, labels))\n",
    "    FN = sum(p == 0 and l == 1 for p, l in zip(predictions, labels))\n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    if TP + FN > 0:\n",
    "        recall = TP / (TP + FN)\n",
    "    else:\n",
    "        recall = 0.0\n",
    "    \n",
    "    if TP + FP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return accuracy, recall, precision, f1_score, {'TP':TP,'TN':TN,'FP':FP,'FN':FN}\n",
    "\n",
    "\n",
    "def calculate_grouped_recall(predicted_peaks, true_peaks, matching_range):\n",
    "    recall_predictions = []\n",
    "\n",
    "    for true_idx in true_peaks:\n",
    "        found_true_pred = False\n",
    "        for pred_idx in predicted_peaks:\n",
    "            if abs(pred_idx - true_idx) <= matching_range:\n",
    "                found_true_pred = True\n",
    "                break\n",
    "        recall_predictions.append(found_true_pred)\n",
    "    \n",
    "\n",
    "    recall = np.sum(recall_predictions) / len(recall_predictions)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def evaluate_classification(predictions, labels):\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    class_report = classification_report(labels, predictions, target_names=[\"Normal\", \"Anomaly\"])\n",
    "    return conf_matrix, class_report\n",
    "\n",
    "\n",
    "def create_evaluation_lists(true_data, predicted_data, data_length, matching_range):\n",
    "    predictions = [0] * data_length\n",
    "    for p in predicted_data:\n",
    "        predictions[p] = 1\n",
    "\n",
    "    labels = [0] * data_length\n",
    "    for p in true_data:\n",
    "        for idx in range(p - matching_range, p + matching_range):\n",
    "            if idx >= 0 and idx < data_length:\n",
    "                labels[idx] = 1\n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "def calculate_evaluation_metrics(data_length, predicted_peaks, true_peaks, matching_range):\n",
    "    predictions, labels = create_evaluation_lists(true_peaks, predicted_peaks, data_length, matching_range)\n",
    "    accuracy, recall, precision, f1_score, conf_mat_metrics = calculate_metrics(predictions, labels)\n",
    "    grouped_recall = calculate_grouped_recall(predicted_peaks, true_peaks, matching_range)\n",
    "    return accuracy, recall, precision, f1_score, grouped_recall, conf_mat_metrics\n",
    "\n",
    "def evaluate_results(all_results, matching_range = 30):\n",
    "    avg_recall, avg_precision, avg_accuracy, avg_f1_score = 0, 0, 0, 0\n",
    "    all_recall, all_precision, all_accuracy, all_f1_score = [], [], [], []\n",
    "    \n",
    "    # If any prediction matches\n",
    "    avg_grouped_recall = 0\n",
    "    all_grouped_recall, all_conf_mat_metrics = [], []\n",
    "\n",
    "    for res in all_results:\n",
    "        predicted_peaks = res['predicted_index']\n",
    "        true_peaks = res['true_index']\n",
    "        data_length = res['data_length']\n",
    "        \n",
    "        accuracy, recall, precision, f1_score, grouped_recall, conf_mat_metrics = calculate_evaluation_metrics(data_length,\n",
    "                                                                                                               predicted_peaks,\n",
    "                                                                                                               true_peaks,\n",
    "                                                                                                               matching_range)\n",
    "        all_recall.append(recall)\n",
    "        all_precision.append(precision)\n",
    "        all_accuracy.append(accuracy)\n",
    "        all_f1_score.append(f1_score)\n",
    "        all_grouped_recall.append(grouped_recall)\n",
    "        all_conf_mat_metrics.append(conf_mat_metrics)\n",
    "        \n",
    "        avg_recall += recall\n",
    "        avg_precision += precision\n",
    "        avg_accuracy += accuracy\n",
    "        avg_f1_score += precision\n",
    "        avg_grouped_recall += grouped_recall\n",
    "\n",
    "    avg_recall /= len(all_results)\n",
    "    avg_precision /= len(all_results)\n",
    "    avg_accuracy /= len(all_results)\n",
    "    avg_f1_score /= len(all_results)\n",
    "    avg_grouped_recall /= len(all_results)\n",
    "    \n",
    "    return {'avg_metrics' : {'avg_accuracy' : avg_accuracy,\n",
    "                             'avg_precision' : avg_precision,\n",
    "                             'avg_recall' : avg_recall,\n",
    "                             'avg_f1_score' : avg_f1_score,\n",
    "                             'avg_grouped_recall' : avg_grouped_recall},\n",
    "            'all_metrics' : {'all_accuracy' : all_accuracy,\n",
    "                             'all_precision' : all_precision,\n",
    "                             'all_recall' : all_recall,\n",
    "                             'all_f1_score' : all_f1_score,\n",
    "                             'all_grouped_recall' : all_grouped_recall},\n",
    "            'conf_mat_metrics' : all_conf_mat_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_prediction(anomalies, grouped_range=60):\n",
    "    idx_group = []\n",
    "    all_groups = []\n",
    "    for idx in list(anomalies.index):\n",
    "        if idx in idx_group:\n",
    "            continue\n",
    "        idx_group = []\n",
    "        for i in range(idx, idx+grouped_range):\n",
    "            if i in list(anomalies.index):\n",
    "                idx_group.append(i)\n",
    "        all_groups.append(idx_group)\n",
    "    \n",
    "    group_pred_idx = []\n",
    "    group_pred_val = []\n",
    "    for group in all_groups:\n",
    "        group_vals = anomalies.loc[group]['distance.origin-prob']\n",
    "        if len(group_vals) > 1 and group_vals.iloc[0] < group_vals.iloc[1]:\n",
    "            pred = group_vals[group_vals == group_vals.max()]\n",
    "        else:\n",
    "            pred = group_vals[group_vals == group_vals.min()]\n",
    "        group_pred_idx.append(pred.index.values[0])\n",
    "        group_pred_val.append(pred.values[0])\n",
    "    return group_pred_idx, group_pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fly Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlyInfo:\n",
    "    def __init__(self, name, trial_id, peak_index, peak_values):\n",
    "        self.name = name\n",
    "        self.trial_id = trial_id\n",
    "        self.peak_index = peak_index\n",
    "        self.peak_values = peak_values\n",
    "\n",
    "class FlyDatabase:\n",
    "    def __init__(self):\n",
    "        self.fly_data = []\n",
    "\n",
    "    def add_fly(self, fly_info):\n",
    "        self.fly_data.append(fly_info)\n",
    "\n",
    "    def get_fly(self, name, trial_id):\n",
    "        for fly_info in self.fly_data:\n",
    "            if fly_info.name == name and fly_info.trial_id == trial_id:\n",
    "                return fly_info\n",
    "        return None\n",
    "    \n",
    "    def write_fly_info(self, name, trial_id):\n",
    "        for fly_info in self.fly_data:\n",
    "            if fly_info.name == name and fly_info.trial_id == trial_id:\n",
    "                print('Name:', fly_info.name)\n",
    "                print('Trial Id:', fly_info.trial_id)\n",
    "                print('Peak Index:', fly_info.peak_index)\n",
    "                print('Peak Values:', fly_info.peak_values)\n",
    "                return None\n",
    "        print('Fly not found!!!')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Fly05182022_5d\n",
      "Trial Id: 0\n",
      "Peak Index: [ 534  694  903 1207 1623]\n",
      "Peak Values: [1014.36112044 1017.73376888 1027.22816309 1021.72159098 1023.54593544]\n"
     ]
    }
   ],
   "source": [
    "fly_db = FlyDatabase()\n",
    "\n",
    "fly_names = true_peak_annotations_df['name'].unique()\n",
    "\n",
    "for name in fly_names:\n",
    "    trial_idxs = true_peak_annotations_df[true_peak_annotations_df['name'] == name]['trial_id'].unique().tolist()\n",
    "    for idx in trial_idxs:\n",
    "        peak_index = true_peak_annotations_df[(true_peak_annotations_df['name'] == name) & (true_peak_annotations_df['trial_id'] == idx)]['peak_index'].values\n",
    "        peak_values = true_peak_annotations_df[(true_peak_annotations_df['name'] == name) & (true_peak_annotations_df['trial_id'] == idx)]['value'].values\n",
    "        fly_db.add_fly(FlyInfo(name, idx, peak_index, peak_values))\n",
    "\n",
    "fly_db.write_fly_info('Fly05182022_5d', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.20264069768278617, Min: 0.018484288354898338, Max: 0.6469002695417789\n"
     ]
    }
   ],
   "source": [
    "peak_ratios = []\n",
    "for fly in fly_db.fly_data:\n",
    "    data_length = len(bouts_dict[fly.name].loc[int(fly.trial_id)]['distance.origin-prob'])\n",
    "    peak_amount = len(fly.peak_index) * 60\n",
    "    peak_ratios.append(peak_amount / data_length)\n",
    "\n",
    "peak_ratio = np.mean(peak_ratios)\n",
    "max_ratio = np.max(peak_ratios)\n",
    "min_ratio = np.min(peak_ratios)\n",
    "\n",
    "print(f'Ratio: {peak_ratio}, Min: {min_ratio}, Max: {max_ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = ['distance.origin-prob',\n",
    "                  'distance.head-prob',\n",
    "                  'pose.prob_x',\n",
    "                  'pose.prob_y']\n",
    "\n",
    "all_results = []\n",
    "all_results_group = []\n",
    "\n",
    "for fly in fly_db.fly_data:\n",
    "    input_data = create_input_data(fly = fly.name,\n",
    "                                   experiment = int(fly.trial_id),\n",
    "                                   features = model_features)\n",
    "    \n",
    "    info_df = pd.DataFrame(input_data, columns = model_features)\n",
    "    scaled_data = StandardScaler().fit_transform(input_data)\n",
    "    model = IsolationForest(n_estimators = 100,\n",
    "                            contamination=0.04,\n",
    "                            max_samples='auto',\n",
    "                            max_features=1.0,\n",
    "                            bootstrap=False)\n",
    "    info_df['predictions'] = model.fit_predict(scaled_data)\n",
    "    anomalies = info_df.loc[info_df['predictions'] == -1, ['distance.origin-prob']]\n",
    "    anomalies_idx = list(info_df.loc[info_df['predictions'] == -1, ['distance.origin-prob']].index)\n",
    "\n",
    "    group_pred_idx, group_pred_val = filter_prediction(anomalies, grouped_range=60)\n",
    "    \n",
    "    all_results.append({'true_index': fly.peak_index, 'predicted_index': anomalies_idx, 'data_length': len(info_df['predictions'].values)})\n",
    "    all_results_group.append({'true_index': fly.peak_index, 'predicted_index': group_pred_idx, 'data_length': len(info_df['predictions'].values)})\n",
    "\n",
    "results = evaluate_results(all_results)\n",
    "results_g = evaluate_results(all_results_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_accuracy': 0.8193176491316547,\n",
       " 'avg_precision': 0.7621257926868178,\n",
       " 'avg_recall': 0.19843957468929802,\n",
       " 'avg_f1_score': 0.7621257926868178,\n",
       " 'avg_grouped_recall': 0.8214927332968859}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['avg_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_accuracy': 0.7999980275830455,\n",
       " 'avg_precision': 0.7557723485037828,\n",
       " 'avg_recall': 0.013715486453742442,\n",
       " 'avg_f1_score': 0.7557723485037828,\n",
       " 'avg_grouped_recall': 0.8030107324650999}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_g['avg_metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deneme Alani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = ['distance.origin-prob',\n",
    "                  'distance.head-prob',\n",
    "                  'pose.prob_x',\n",
    "                  'pose.prob_y']\n",
    "\n",
    "fly = fly_db.fly_data[0]\n",
    "\n",
    "input_data = create_input_data(fly = fly.name,\n",
    "                                   experiment = int(fly.trial_id),\n",
    "                                   features = model_features)\n",
    "    \n",
    "info_df = pd.DataFrame(input_data, columns = model_features)\n",
    "scaled_data = StandardScaler().fit_transform(input_data)\n",
    "model = IsolationForest(n_estimators = 100,\n",
    "                        contamination=0.04,\n",
    "                        max_samples='auto',\n",
    "                        max_features=1.0,\n",
    "                        bootstrap=False)\n",
    "info_df['predictions'] = model.fit_predict(scaled_data)\n",
    "anomalies = info_df.loc[info_df['predictions'] == -1, ['distance.origin-prob']]\n",
    "anomalies_idx = list(info_df.loc[info_df['predictions'] == -1, ['distance.origin-prob']].index)\n",
    "\n",
    "group_pred_idx, group_pred_val = filter_prediction(anomalies, grouped_range=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 43 TN 1818 FP 44 FN 257\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_results([{'true_index': fly.peak_index,\n",
    "                            'predicted_index': anomalies_idx,\n",
    "                            'all_preds': info_df['predictions'].values.tolist()}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_accuracy': 0.8607770582793709,\n",
       " 'avg_precision': 0.4942528735632184,\n",
       " 'avg_recall': 0.14333333333333334,\n",
       " 'avg_f1_score': 0.4942528735632184,\n",
       " 'avg_grouped_recall': 1.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['avg_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 5 TN 1860 FP 2 FN 295\n"
     ]
    }
   ],
   "source": [
    "results_g = evaluate_results([{'true_index': fly.peak_index,\n",
    "                            'predicted_index': group_pred_idx,\n",
    "                            'all_preds': info_df['predictions'].values.tolist()}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_accuracy': 0.862627197039778,\n",
       " 'avg_precision': 0.7142857142857143,\n",
       " 'avg_recall': 0.016666666666666666,\n",
       " 'avg_f1_score': 0.7142857142857143,\n",
       " 'avg_grouped_recall': 1.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_g['avg_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
