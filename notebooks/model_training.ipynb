{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.progress import TQDMProgressBar\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "import pytorchvideo.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit Dataset\n",
    "\n",
    " To ensure a constant number of samples are retrieved from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fly Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlyDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        super().__init__()\n",
    "\n",
    "    def _make_transforms(self, mode: str):\n",
    "        return Compose([self._video_transform(mode)])\n",
    "\n",
    "    def _video_transform(self, mode: str):\n",
    "        return ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(self.args[\"num_frames_to_sample\"]),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(self.args[\"video_means\"], self.args[\"video_stds\"]),\n",
    "                ]\n",
    "                + (\n",
    "                    [\n",
    "                        RandomShortSideScale(\n",
    "                            min_size=self.args[\"video_min_short_side_scale\"],\n",
    "                            max_size=self.args[\"video_max_short_side_scale\"],\n",
    "                        ),\n",
    "                        RandomCrop(self.args[\"crop_size\"]),\n",
    "                    ]\n",
    "                    if mode == \"train\"\n",
    "                    else [\n",
    "                        ShortSideScale(self.args[\"video_min_short_side_scale\"]),\n",
    "                        CenterCrop(self.args[\"crop_size\"]),\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        train_transform = self._make_transforms(mode=\"train\")\n",
    "\n",
    "        self.train_dataset = LimitDataset(\n",
    "            pytorchvideo.data.labeled_video_dataset(\n",
    "                data_path=self.args[\"train_data_path\"],\n",
    "                clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', self.args[\"clip_duration\"]), # Experiment olarak random da denenebilir\n",
    "                transform=train_transform,\n",
    "                video_path_prefix=self.args[\"video_path_prefix\"], # could be '' I think\n",
    "                decode_audio=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        val_transform = self._make_transforms(mode=\"val\")\n",
    "\n",
    "        self.val_dataset = LimitDataset(\n",
    "            pytorchvideo.data.labeled_video_dataset(\n",
    "                data_path=self.args[\"val_data_path\"],\n",
    "                clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', self.args[\"clip_duration\"]), # Experiment olarak random da denenebilir\n",
    "                transform=val_transform,\n",
    "                video_path_prefix=self.args[\"video_path_prefix\"], # could be '' I think\n",
    "                decode_audio=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.args[\"batch_size\"]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassificationLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        self.dataloader_length = 0\n",
    "        self.classes = ['Feeding', 'Grooming', 'Pumping']\n",
    "\n",
    "        self.save_hyperparameters(\"args\")\n",
    "        \n",
    "        # For logging outputs\n",
    "        self.epoch_logits = []\n",
    "        self.epoch_incorrect_samples = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, stage='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, stage='val')\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx, stage='train'):\n",
    "        X, y = batch['video'], batch['label']\n",
    "\n",
    "        output = self(X.permute(0, 2, 1, 3, 4)) # (8, 3, 16, 224, 224) -> (8, 16, 3, 224, 224)\n",
    "\n",
    "        loss = F.cross_entropy(output.logits, y)\n",
    "        acc = torchmetrics.functional.accuracy(output.logits, y, task=\"multiclass\", num_classes=3)\n",
    "\n",
    "        self.log(\n",
    "            f\"{stage}_loss\", loss.item(), batch_size=self.args[\"batch_size\"], on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{stage}_acc\", acc, batch_size=self.args[\"batch_size\"], on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        if stage == 'train':\n",
    "            return loss\n",
    "        elif stage == 'val':\n",
    "            _, predictions = torch.max(output.logits, dim=1)\n",
    "            incorrect_samples = X[predictions != y]  # Get incorrect samples\n",
    "            self.epoch_logits.extend(output.logits)\n",
    "            if len(self.epoch_incorrect_samples) < 1:\n",
    "                self.epoch_incorrect_samples.extend(incorrect_samples[0])\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        #dummy_input = torch.zeros((1, 8, 3, 224, 224), device=self.device)\n",
    "        #model_filename = \"model_ckpt.onnx\"\n",
    "        #torch.onnx.export(self, dummy_input, model_filename, opset_version=11)\n",
    "        #artifact = wandb.Artifact(name=\"model.ckpt\", type=\"model\")\n",
    "        #artifact.add_file(model_filename)\n",
    "        #self.logger.experiment.log_artifact(artifact)\n",
    "\n",
    "        flattened_logits = torch.flatten(torch.cat(self.epoch_logits))\n",
    "        incorrect_samples = self.epoch_incorrect_samples[0]\n",
    "\n",
    "        print('false_predictions:', incorrect_samples.shape)\n",
    "        print('logits:', flattened_logits)\n",
    "        print('global_step:', self.global_step)\n",
    "        \n",
    "        self.epoch_logits.clear()\n",
    "        self.epoch_incorrect_samples.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.args[\"lr\"],\n",
    "        )\n",
    "        return [optimizer]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor_config(model, image_processor, sample_rate=8, fps=30):\n",
    "\n",
    "    mean = image_processor.image_mean\n",
    "    std = image_processor.image_std\n",
    "\n",
    "    if \"shortest_edge\" in image_processor.size:\n",
    "        height = width = image_processor.size[\"shortest_edge\"]\n",
    "    else:\n",
    "        height = image_processor.size[\"height\"]\n",
    "        width = image_processor.size[\"width\"]\n",
    "\n",
    "    crop_size = (height, width)\n",
    "\n",
    "    num_frames_to_sample = model.config.num_frames # 16 for VideoMAE\n",
    "    clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "    print('Clip Duration:', clip_duration, 'seconds')\n",
    "\n",
    "    return {\n",
    "        \"image_mean\" : mean,\n",
    "        \"image_std\" : std,\n",
    "        \"crop_size\" : crop_size,\n",
    "        \"num_frames_to_sample\" : num_frames_to_sample,\n",
    "        \"clip_duration\": clip_duration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timesformer_model(ckpt, label2id, id2label, num_frames):\n",
    "    return TimesformerForVideoClassification.from_pretrained(\n",
    "        ckpt,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "        num_frames = num_frames\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading pytorch_model.bin: 100%|██████████| 486M/486M [05:18<00:00, 1.52MB/s]\n",
      "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip Duration: 4.266666666666667 seconds\n"
     ]
    }
   ],
   "source": [
    "# PATH INFO\n",
    "PROJ_DIR = '/Users/mpekey/Desktop/FlyVideo'\n",
    "TRAIN_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Train')\n",
    "VAL_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Validation')\n",
    "\n",
    "\n",
    "# DATASET INFO\n",
    "class_labels = ['Feeding', 'Grooming', 'Pumping']\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = get_timesformer_model(ckpt=\"facebook/timesformer-base-finetuned-k400\",\n",
    "                                  label2id=label2id,\n",
    "                                  id2label=id2label,\n",
    "                                  num_frames=8)\n",
    "\n",
    "# Freeze the model    \n",
    "for param in model.timesformer.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip Duration: 4.266666666666667 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create Arguments\n",
    "model_args = create_preprocessor_config(model, \n",
    "                                        image_processor, \n",
    "                                        sample_rate=16, \n",
    "                                        fps=30)\n",
    "\n",
    "args = {\n",
    "    # Data\n",
    "    \"train_data_path\" : TRAIN_DATA_PATH,\n",
    "    \"val_data_path\" : VAL_DATA_PATH,\n",
    "    \"lr\" : 0.001,\n",
    "    \"max_epochs\" : 1,\n",
    "    \"batch_size\" : 16,\n",
    "    \"video_path_prefix\" : '',\n",
    "    \"video_min_short_side_scale\" : 256,\n",
    "    \"video_max_short_side_scale\" : 320,\n",
    "    \"clip_duration\" : model_args[\"clip_duration\"],\n",
    "    \"crop_size\" : model_args[\"crop_size\"],\n",
    "    \"num_frames_to_sample\": model_args[\"num_frames_to_sample\"],\n",
    "    \"video_means\" : model_args[\"image_mean\"],\n",
    "    \"video_stds\" : model_args[\"image_std\"],\n",
    "    \"sample_rate\": 16,\n",
    "    \"fps\":30,\n",
    "    \"num_frames\":8\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/mpekey/Desktop/FlyVideo/notebooks/lightning_logs\n",
      "\n",
      "  | Name  | Type                              | Params\n",
      "------------------------------------------------------------\n",
      "0 | model | TimesformerForVideoClassification | 121 M \n",
      "------------------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "121 M     Non-trainable params\n",
      "121 M     Total params\n",
      "485.044   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "        max_epochs=args[\"max_epochs\"],\n",
    "        callbacks=[TQDMProgressBar(refresh_rate=args[\"batch_size\"])],\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"auto\",\n",
    "        log_every_n_steps=40\n",
    "    )\n",
    "classification_module = VideoClassificationLightningModule(model, args)\n",
    "data_module = FlyDataModule(args)\n",
    "trainer.fit(classification_module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
