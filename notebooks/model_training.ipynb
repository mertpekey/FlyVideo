{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH INFO\n",
    "PROJ_DIR = '/Users/mpekey/Desktop/FlyVideo'\n",
    "TRAIN_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Train')\n",
    "VAL_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Validation')\n",
    "\n",
    "# MODEL INFO\n",
    "MODEL_CHECKPOINT = \"MCG-NJU/videomae-base\"\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# DATASET INFO\n",
    "class_labels = ['Feeding', 'Grooming', 'Pumping']\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "    num_frames = 16 # Default is 16\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "\n",
    "crop_size = (height, width)\n",
    "\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames # 16 for VideoMAE\n",
    "sample_rate = 8\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "print('Clip Duration:', clip_duration, 'seconds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentations\n",
    "\n",
    "Train ve Val ayri olacak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_transforms = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    RandomCrop(crop_size),\n",
    "                    #RandomHorizontalFlip(p=0.5)\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dataset\n",
    "\n",
    "Train ve Val ayri olacak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fly_dataset = pytorchvideo.data.labeled_video_dataset(data_path=DATA_PATH,\n",
    "                                                      clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', clip_duration),\n",
    "                                                      transform=basic_transforms,\n",
    "                                                      video_path_prefix='',\n",
    "                                                      decode_audio=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit Dataset\n",
    "\n",
    " To ensure a constant number of samples are retrieved from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fly Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlyDataModule(pytorch_lightning.LightningDataModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        super().__init__()\n",
    "\n",
    "    def _make_transforms(self, mode: str):\n",
    "        return Compose(self._video_transform(mode))\n",
    "\n",
    "    def _video_transform(self, mode: str):\n",
    "        return ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(self.args.video_num_subsampled),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(self.args.video_means, self.args.video_stds),\n",
    "                ]\n",
    "                + (\n",
    "                    [\n",
    "                        RandomShortSideScale(\n",
    "                            min_size=self.args.video_min_short_side_scale,\n",
    "                            max_size=self.args.video_max_short_side_scale,\n",
    "                        ),\n",
    "                        RandomCrop(self.args.video_crop_size),\n",
    "                    ]\n",
    "                    if mode == \"train\"\n",
    "                    else [\n",
    "                        ShortSideScale(self.args.video_min_short_side_scale),\n",
    "                        CenterCrop(self.args.video_crop_size),\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        train_transform = self._make_transforms(mode=\"train\")\n",
    "\n",
    "        self.train_dataset = LimitDataset(\n",
    "            pytorchvideo.data.labeled_video_dataset(\n",
    "                data_path=TRAIN_DATA_PATH,\n",
    "                clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', self.args.clip_duration), # Experiment olarak random da denenebilir\n",
    "                transform=train_transform,\n",
    "                video_path_prefix=self.args.video_path_prefix, # could be '' I think\n",
    "                decode_audio=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        val_transform = self._make_transforms(mode=\"val\")\n",
    "\n",
    "        self.val_dataset = LimitDataset(\n",
    "            pytorchvideo.data.labeled_video_dataset(\n",
    "                data_path=VAL_DATA_PATH,\n",
    "                clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', self.args.clip_duration), # Experiment olarak random da denenebilir\n",
    "                transform=val_transform,\n",
    "                video_path_prefix=self.args.video_path_prefix, # could be '' I think\n",
    "                decode_audio=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.args.batch_size\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.args = args\n",
    "        super().__init__()\n",
    "\n",
    "        # Model\n",
    "        self.model = pytorchvideo.models.resnet.create_resnet(\n",
    "            input_channel=3,\n",
    "            model_num_class=400,\n",
    "        )\n",
    "\n",
    "        # Metrics\n",
    "        self.train_accuracy = pytorch_lightning.metrics.Accuracy()\n",
    "        self.val_accuracy = pytorch_lightning.metrics.Accuracy()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(self, batch, batch_idx, stage='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Should return or not in validation ???\n",
    "        return self._common_step(self, batch, batch_idx, stage='train')\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx, stage='train'):\n",
    "        X, y = batch['video'], batch['label']\n",
    "\n",
    "        y_pred = self.model(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        acc = self.val_accuracy(F.softmax(y_pred, dim=-1), y)\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss)\n",
    "        self.log(\n",
    "            f\"{stage}_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.args.lr,\n",
    "            momentum=self.args.momentum,\n",
    "            weight_decay=self.args.weight_decay,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, self.args.max_epochs, last_epoch=-1\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pytorch_lightning.Trainer.from_argparse_args(args)\n",
    "classification_module = VideoClassificationLightningModule(args)\n",
    "data_module = FlyDataModule(args)\n",
    "trainer.fit(classification_module, data_module)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
