{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cta/users/mpekey/miniconda3/envs/lightning/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.progress import TQDMProgressBar\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "import pytorchvideo.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import wandb\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit Dataset\n",
    "\n",
    " To ensure a constant number of samples are retrieved from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fly Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlyDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        super().__init__()\n",
    "\n",
    "    def _make_transforms(self, mode: str):\n",
    "        return Compose([self._video_transform(mode)])\n",
    "\n",
    "    def _video_transform(self, mode: str):\n",
    "        return ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(self.args[\"num_frames_to_sample\"]),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(self.args[\"video_means\"], self.args[\"video_stds\"]),\n",
    "                ]\n",
    "                + (\n",
    "                    [\n",
    "                        RandomShortSideScale(\n",
    "                            min_size=self.args[\"video_min_short_side_scale\"],\n",
    "                            max_size=self.args[\"video_max_short_side_scale\"],\n",
    "                        ),\n",
    "                        RandomCrop(self.args[\"crop_size\"]),\n",
    "                    ]\n",
    "                    if mode == \"train\"\n",
    "                    else [\n",
    "                        ShortSideScale(self.args[\"video_min_short_side_scale\"]),\n",
    "                        CenterCrop(self.args[\"crop_size\"]),\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        train_transform = self._make_transforms(mode=\"train\")\n",
    "\n",
    "        self.train_dataset = LimitDataset(\n",
    "            pytorchvideo.data.labeled_video_dataset(\n",
    "                data_path=self.args[\"train_data_path\"],\n",
    "                clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', self.args[\"clip_duration\"]), # Experiment olarak random da denenebilir\n",
    "                transform=train_transform,\n",
    "                video_path_prefix=self.args[\"video_path_prefix\"], # could be '' I think\n",
    "                decode_audio=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        val_transform = self._make_transforms(mode=\"val\")\n",
    "\n",
    "        self.val_dataset = LimitDataset(\n",
    "            pytorchvideo.data.labeled_video_dataset(\n",
    "                data_path=self.args[\"val_data_path\"],\n",
    "                clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', self.args[\"clip_duration\"]), # Experiment olarak random da denenebilir\n",
    "                transform=val_transform,\n",
    "                video_path_prefix=self.args[\"video_path_prefix\"], # could be '' I think\n",
    "                decode_audio=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.args[\"batch_size\"]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassificationLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        self.dataloader_length = 0\n",
    "        self.classes = ['Feeding', 'Grooming', 'Pumping']\n",
    "\n",
    "        self.save_hyperparameters(\"args\")\n",
    "        \n",
    "        # For logging outputs\n",
    "        self.epoch_logits = []\n",
    "        self.epoch_incorrect_samples = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, stage='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, stage='val')\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx, stage='train'):\n",
    "        X, y = batch['video'], batch['label']\n",
    "\n",
    "        output = self(X.permute(0, 2, 1, 3, 4)) # (8, 3, 16, 224, 224) -> (8, 16, 3, 224, 224)\n",
    "\n",
    "        loss = F.cross_entropy(output.logits, y)\n",
    "        acc = torchmetrics.functional.accuracy(output.logits, y, task=\"multiclass\", num_classes=3)\n",
    "\n",
    "        self.log(\n",
    "            f\"{stage}_loss\", loss.item(), batch_size=self.args[\"batch_size\"], on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{stage}_acc\", acc, batch_size=self.args[\"batch_size\"], on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        if stage == 'train':\n",
    "            return loss\n",
    "        elif stage == 'val':\n",
    "            _, predictions = torch.max(output.logits, dim=1)\n",
    "            incorrect_samples = X.permute(0, 2, 1, 3, 4)[predictions != y]  # Get incorrect samples\n",
    "            self.epoch_logits.extend(output.logits)\n",
    "            if self.epoch_incorrect_samples is None:\n",
    "                self.epoch_incorrect_samples = incorrect_samples[0]\n",
    "                print(len(incorrect_samples))\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        #dummy_input = torch.zeros((1, 8, 3, 224, 224), device=self.device)\n",
    "        #model_filename = \"model_ckpt.onnx\"\n",
    "        #torch.onnx.export(self, dummy_input, model_filename, opset_version=11)\n",
    "        #artifact = wandb.Artifact(name=\"model.ckpt\", type=\"model\")\n",
    "        #artifact.add_file(model_filename)\n",
    "        #self.logger.experiment.log_artifact(artifact)\n",
    "\n",
    "        flattened_logits = torch.flatten(torch.cat(self.epoch_logits))\n",
    "        incorrect_samples = self.epoch_incorrect_samples\n",
    "\n",
    "        print('false_predictions:', incorrect_samples.shape)\n",
    "        print('logits:', flattened_logits)\n",
    "        print('global_step:', self.global_step)\n",
    "        \n",
    "        self.epoch_logits.clear()\n",
    "        self.epoch_incorrect_samples=None\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.args[\"lr\"],\n",
    "        )\n",
    "        return [optimizer]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor_config(model, image_processor, sample_rate=8, fps=30):\n",
    "\n",
    "    mean = image_processor.image_mean\n",
    "    std = image_processor.image_std\n",
    "\n",
    "    if \"shortest_edge\" in image_processor.size:\n",
    "        height = width = image_processor.size[\"shortest_edge\"]\n",
    "    else:\n",
    "        height = image_processor.size[\"height\"]\n",
    "        width = image_processor.size[\"width\"]\n",
    "\n",
    "    crop_size = (height, width)\n",
    "\n",
    "    num_frames_to_sample = model.config.num_frames # 16 for VideoMAE\n",
    "    clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "    print('Clip Duration:', clip_duration, 'seconds')\n",
    "\n",
    "    return {\n",
    "        \"image_mean\" : mean,\n",
    "        \"image_std\" : std,\n",
    "        \"crop_size\" : crop_size,\n",
    "        \"num_frames_to_sample\" : num_frames_to_sample,\n",
    "        \"clip_duration\": clip_duration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timesformer_model(ckpt, label2id, id2label, num_frames):\n",
    "    return TimesformerForVideoClassification.from_pretrained(\n",
    "        ckpt,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "        num_frames = num_frames\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# PATH INFO\n",
    "PROJ_DIR = '/cta/users/mpekey/FlyVideo'\n",
    "TRAIN_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Train')\n",
    "VAL_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Validation')\n",
    "\n",
    "\n",
    "# DATASET INFO\n",
    "class_labels = ['Feeding', 'Grooming', 'Pumping']\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = get_timesformer_model(ckpt=\"facebook/timesformer-base-finetuned-k400\",\n",
    "                                  label2id=label2id,\n",
    "                                  id2label=id2label,\n",
    "                                  num_frames=8)\n",
    "\n",
    "# Freeze the model    \n",
    "for param in model.timesformer.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_DIR = '/cta/users/mpekey/FlyVideo'\n",
    "TRAIN_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Train')\n",
    "VAL_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip Duration: 4.266666666666667 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create Arguments\n",
    "model_args = create_preprocessor_config(model, \n",
    "                                        image_processor, \n",
    "                                        sample_rate=16, \n",
    "                                        fps=30)\n",
    "\n",
    "args = {\n",
    "    # Data\n",
    "    \"train_data_path\" : TRAIN_DATA_PATH,\n",
    "    \"val_data_path\" : VAL_DATA_PATH,\n",
    "    \"lr\" : 0.001,\n",
    "    \"max_epochs\" : 1,\n",
    "    \"batch_size\" : 16,\n",
    "    \"video_path_prefix\" : '',\n",
    "    \"video_min_short_side_scale\" : 256,\n",
    "    \"video_max_short_side_scale\" : 320,\n",
    "    \"clip_duration\" : model_args[\"clip_duration\"],\n",
    "    \"crop_size\" : model_args[\"crop_size\"],\n",
    "    \"num_frames_to_sample\": model_args[\"num_frames_to_sample\"],\n",
    "    \"video_means\" : model_args[\"image_mean\"],\n",
    "    \"video_stds\" : model_args[\"image_std\"],\n",
    "    \"sample_rate\": 16,\n",
    "    \"fps\":30,\n",
    "    \"num_frames\":8\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type                              | Params\n",
      "------------------------------------------------------------\n",
      "0 | model | TimesformerForVideoClassification | 121 M \n",
      "------------------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "121 M     Non-trainable params\n",
      "121 M     Total params\n",
      "485.044   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]13\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:36<00:00, 18.27s/it]false_predictions: torch.Size([8, 3, 224, 224])\n",
      "logits: tensor([ 0.6479,  0.8090, -0.2530,  0.5467,  0.6009, -0.0518,  0.4856,  0.3367,\n",
      "         0.1013,  0.4477,  0.1497,  0.1963,  0.3785,  0.3198,  0.6934,  0.5756,\n",
      "         0.4119, -0.2066,  0.4649,  0.5531, -0.4300,  0.8923,  0.3700, -0.2273,\n",
      "         0.8465,  0.3662, -0.0679,  0.7400,  0.2328,  0.1603,  0.5228,  0.4122,\n",
      "        -0.0802,  0.4988,  0.4221, -0.0790,  0.5141,  0.3994, -0.1032,  0.5178,\n",
      "         0.4667, -0.0360,  0.4729,  0.4105, -0.1231,  0.4547,  0.3870, -0.0892,\n",
      "         0.6912,  0.4379, -0.6753,  0.8330,  0.1755, -0.3654,  0.8428,  0.0978,\n",
      "        -0.3756, -0.2428,  0.3205, -0.3050, -0.2836,  0.3562, -0.3126, -0.2763,\n",
      "         0.3484, -0.3535, -0.4278,  0.2953, -0.3705, -0.3642,  0.3027, -0.3779,\n",
      "         0.2304,  0.7050,  0.0158,  0.1370,  0.4829,  0.1189,  0.3913,  0.3473,\n",
      "         0.2263,  0.2215,  0.4367,  0.1960,  0.2322,  0.4160,  0.1200,  0.2056,\n",
      "         0.4857, -0.0329,  0.0424,  0.5613,  0.1396,  0.1875,  0.4179,  0.2729],\n",
      "       device='cuda:0')\n",
      "global_step: 0\n",
      "Epoch 0: 100%|██████████| 40/40 [24:58<00:00, 37.47s/it, v_num=4]          1\n",
      "false_predictions: torch.Size([8, 3, 224, 224])\n",
      "logits: tensor([ 1.3290e+00,  6.0296e-01, -2.1865e+00,  1.3366e+00, -2.2481e-01,\n",
      "        -1.6369e+00,  8.7269e-01,  2.4148e-02, -4.6622e-01,  1.0396e+00,\n",
      "         2.2158e-01, -7.7389e-01,  1.3413e+00,  2.7485e-01, -1.0436e+00,\n",
      "         5.3364e-01,  3.4104e-01,  2.4571e-01,  7.4973e-01,  2.5793e-01,\n",
      "        -2.6619e-01, -5.4521e-01,  4.8951e-01, -1.3749e+00, -7.6314e-01,\n",
      "         4.8218e-01,  6.0196e-01, -6.6136e-01,  4.6160e-01,  5.5748e-01,\n",
      "        -6.7636e-01,  4.7665e-01,  4.8286e-01, -6.8382e-01,  3.5928e-01,\n",
      "         5.8131e-01, -7.0893e-01,  3.6603e-01,  7.3118e-01, -6.9437e-01,\n",
      "         3.4118e-01,  7.9238e-01,  6.0049e-01,  6.4767e-01, -1.0876e+00,\n",
      "        -3.5312e-01,  1.8637e+00, -1.5735e+00, -4.7947e-01,  2.0103e+00,\n",
      "        -1.2006e+00, -5.0255e-01,  1.9405e+00, -1.3087e+00,  2.8168e-01,\n",
      "         2.5055e-02, -5.6237e-01,  1.4608e+00, -1.0780e-01, -1.0668e+00,\n",
      "         3.8722e-01, -3.4848e-01, -4.1463e-02,  1.4243e+00, -2.1413e-01,\n",
      "        -8.6945e-01,  1.5391e+00,  3.6603e-01, -2.1964e+00,  1.3216e+00,\n",
      "        -8.5663e-02, -1.1682e+00,  7.4841e-01, -2.1395e-01, -5.4061e-01,\n",
      "         1.7314e+00, -5.8397e-04, -1.9670e+00,  1.6665e+00, -4.4479e-03,\n",
      "        -1.4326e+00,  1.3258e+00,  4.0623e-01, -1.7555e+00,  9.2640e-01,\n",
      "         2.0062e-01, -1.2702e+00,  1.4516e+00,  1.5720e-01, -1.1909e+00,\n",
      "         1.4013e+00,  6.3569e-01, -1.6076e+00, -2.4104e-02,  7.8799e-01,\n",
      "        -1.7931e+00,  6.7063e-01,  1.5903e+00, -2.4574e+00, -6.9335e-01,\n",
      "         1.8048e+00, -1.1302e+00, -8.4671e-01, -6.0104e-01,  4.4738e-01,\n",
      "        -8.3127e-01, -6.2326e-01,  4.0701e-01, -8.3508e-01, -5.8746e-01,\n",
      "         5.3252e-01, -7.7422e-01, -6.7273e-01,  6.8497e-01, -8.0867e-01,\n",
      "        -6.6733e-01,  7.7101e-01, -7.2880e-01, -7.5909e-01,  8.4018e-01,\n",
      "        -7.0473e-01, -7.1795e-01,  7.1224e-01, -2.8078e-02,  5.6975e-01,\n",
      "        -1.8341e-01, -5.2199e-01,  4.3179e-01,  4.7177e-01, -5.9368e-01,\n",
      "         3.6503e-01,  7.1500e-01, -5.7022e-01,  4.4695e-01,  4.2111e-01,\n",
      "        -3.9034e-01,  7.6787e-01, -1.6518e-01, -5.1689e-01,  8.7510e-01,\n",
      "         1.1309e-01, -5.3260e-01,  6.1925e-01,  1.7311e-01, -2.1091e-01,\n",
      "         6.0941e-01,  3.6729e-03, -4.9409e-01,  8.6087e-01, -1.6566e-01,\n",
      "        -5.1430e-01,  6.0887e-01,  8.6089e-01, -5.7943e-01,  5.8568e-01,\n",
      "         9.1163e-01, -5.8480e-01,  6.5372e-01,  8.1128e-01, -5.4195e-01,\n",
      "         5.8184e-01,  9.4210e-01, -5.8048e-01,  6.2996e-01,  9.9480e-01,\n",
      "         1.2916e+00,  1.4435e-01, -1.3189e+00,  7.9841e-01,  1.0601e-02,\n",
      "        -2.8142e-01,  1.3190e+00, -1.9973e-01, -7.1524e-01,  1.6921e-01,\n",
      "        -1.1159e-01,  7.1047e-01,  1.9348e-01, -3.0241e-02,  6.6995e-01,\n",
      "         2.5465e-01,  4.4098e-02,  6.4088e-01,  6.8014e-01,  6.9768e-01,\n",
      "        -1.0485e+00,  1.0017e-01,  1.0973e+00, -2.0051e-01, -9.2256e-01,\n",
      "        -2.8167e-02,  5.9270e-01, -8.6310e-01,  1.1411e-02,  6.1198e-01,\n",
      "        -9.9541e-01,  7.4617e-02,  9.7919e-01, -9.9373e-01,  1.5400e-01,\n",
      "         1.1407e+00, -9.9203e-01,  1.6799e-01,  1.0575e+00, -1.5360e-01,\n",
      "        -1.8530e-01,  4.0523e-01,  6.5957e-01, -1.3651e-01, -4.2091e-01,\n",
      "         6.7257e-01, -2.3256e-01, -3.4716e-01,  6.6374e-01, -1.2314e-01,\n",
      "        -4.2094e-01,  6.2143e-01, -3.3481e-02, -4.8932e-01,  5.6811e-01,\n",
      "        -1.1272e-01, -3.4644e-01,  5.3063e-01, -6.0775e-02, -4.9412e-01,\n",
      "         4.9993e-01, -4.4453e-02, -4.8164e-01,  5.1811e-01, -2.2430e-02,\n",
      "        -4.8486e-01,  6.5603e-01, -4.4667e-02, -5.2523e-01,  5.6494e-01,\n",
      "        -5.1139e-02, -4.5886e-01,  6.0122e-01, -6.8917e-02, -4.7072e-01,\n",
      "         5.0498e-01, -3.2649e-02, -4.6348e-01,  5.4871e-01, -1.5815e-02,\n",
      "        -5.1876e-01,  5.1357e-01,  1.5735e-01, -6.8453e-01,  2.4328e-02,\n",
      "         1.7882e-01, -6.2396e-02,  1.5525e+00, -1.4001e-02, -1.2979e+00,\n",
      "        -3.6172e-01,  4.4528e-01,  5.7728e-01, -5.1585e-01,  4.4457e-01,\n",
      "         7.1293e-01, -5.6756e-01,  4.2494e-01,  8.3890e-01, -8.6199e-01,\n",
      "         1.1344e-02,  1.1264e+00, -7.1724e-01,  1.3674e-02,  9.8820e-01,\n",
      "        -7.7169e-01,  9.9929e-03,  1.0550e+00,  7.3944e-01,  9.4077e-01,\n",
      "        -1.6631e+00, -1.2501e-01,  6.3523e-01, -3.1601e-01, -1.0937e-01,\n",
      "         5.2336e-01, -1.5137e-01, -4.4942e-02,  5.4065e-01, -1.5954e-01,\n",
      "        -1.3201e-02,  5.1621e-01, -2.1026e-01,  3.8184e-01,  1.1323e+00,\n",
      "        -2.2202e-01,  1.2095e+00,  3.4167e-01, -1.2458e+00,  1.0911e+00,\n",
      "         2.7611e-01, -1.0003e+00,  1.2479e+00,  1.6335e-01, -8.4882e-01,\n",
      "         1.5128e+00,  8.6386e-02, -1.3781e+00,  1.7623e+00,  3.2291e-01,\n",
      "        -1.5900e+00,  1.0096e+00,  1.1043e-01, -7.3544e-01, -1.5035e-01,\n",
      "         1.7362e-01, -1.6452e-01, -2.1033e-01,  6.3264e-02, -1.0670e-01,\n",
      "        -1.8381e-02,  1.5934e-01, -3.8141e-01, -1.5041e-01,  1.2805e-01,\n",
      "        -2.9623e-01, -1.1233e-01,  1.8543e-01, -3.6658e-01, -2.3622e-01,\n",
      "         2.0286e-01, -3.2335e-01, -3.3984e-01,  1.2528e-01, -5.7805e-02,\n",
      "        -2.6674e-01,  2.1378e-01, -2.7864e-01, -4.3600e-01, -9.1151e-01,\n",
      "         1.1618e-01, -6.5914e-01, -7.4478e-01,  4.5491e-01,  6.5766e-02,\n",
      "        -5.7657e-02, -9.4571e-01, -1.2908e+00,  2.4190e-01,  1.1135e+00,\n",
      "        -1.3938e+00,  1.6288e-01,  1.4545e+00, -1.4429e+00,  2.6854e-01,\n",
      "         1.5292e+00, -1.3618e+00,  2.9005e-01,  1.4637e+00, -1.3415e+00,\n",
      "         3.7978e-01,  1.3424e+00, -1.3693e+00,  2.6903e-01,  1.4880e+00,\n",
      "         8.6771e-01,  6.9946e-01, -8.3339e-01,  5.2967e-01,  8.2800e-01,\n",
      "        -1.1802e+00, -4.8387e-01,  7.9904e-01, -2.5663e-01, -2.9262e-01,\n",
      "         3.9208e-01,  2.8677e-01, -2.9417e-01,  6.1316e-01,  1.8409e-01,\n",
      "        -3.1316e-01,  8.2991e-01,  4.5301e-02, -7.8792e-02,  5.0252e-01,\n",
      "        -6.5967e-01,  1.8153e-01,  5.7315e-01, -9.8910e-01,  1.3422e-01,\n",
      "        -1.5942e-01,  5.1415e-01,  2.4353e-01, -2.3690e-01,  2.7757e-01,\n",
      "        -3.0005e-01,  2.7116e-01,  1.4328e+00, -2.8591e-01,  2.5375e-01,\n",
      "         1.3915e+00, -2.5588e-01,  2.9250e-01,  1.3718e+00, -4.1379e-01,\n",
      "         2.6339e-01,  1.4653e+00, -2.7138e-01,  3.2585e-01,  1.3071e+00,\n",
      "        -2.3497e-01,  2.8862e-01,  1.1975e+00, -2.9031e-01,  3.2467e-01,\n",
      "         1.2757e+00,  1.4329e+00,  3.5034e-01, -1.7072e+00,  5.2580e-01,\n",
      "         2.7949e-01, -9.1892e-01, -2.9772e-01,  1.0111e+00, -1.5605e-01,\n",
      "        -1.9813e-01,  9.6784e-01, -6.1284e-01, -4.3034e-01,  9.2210e-01,\n",
      "        -3.1331e-01, -4.0806e-01,  9.0261e-01, -4.2255e-02, -4.4996e-01,\n",
      "         6.8226e-01,  7.2777e-02,  1.1294e+00,  3.2399e-01, -1.7983e+00,\n",
      "         5.6213e-01,  5.2573e-01, -8.9631e-01,  5.5543e-01,  4.8851e-01,\n",
      "        -5.9388e-01,  6.0091e-01,  4.8133e-01, -8.3684e-01,  6.9347e-01,\n",
      "         5.1993e-01, -9.0192e-01,  8.2798e-01,  2.8122e-01, -1.0200e+00,\n",
      "         9.5691e-01,  4.0615e-01, -1.3456e+00,  9.4158e-01,  4.3323e-01,\n",
      "        -1.4111e+00, -2.9210e-01,  9.2686e-01, -8.9442e-01,  2.6660e-01,\n",
      "         8.6638e-01, -1.7229e+00,  7.9391e-01,  5.4470e-01, -1.4413e+00,\n",
      "        -1.2599e+00,  1.0531e+00,  1.2339e+00,  1.2134e+00,  2.7886e-01,\n",
      "        -8.3509e-01,  1.6557e+00,  2.8122e-01, -1.7181e+00,  1.5091e+00,\n",
      "         2.4310e-01, -1.4446e+00,  1.6417e+00,  1.9090e-01, -1.0049e+00,\n",
      "         6.2493e-01,  1.3108e+00, -1.7191e+00,  2.3938e-01,  1.3778e+00,\n",
      "        -1.4045e+00,  5.4600e-02,  1.4310e+00, -6.8574e-01,  4.4878e-02,\n",
      "         1.3784e+00, -4.4216e-01,  5.9349e-01,  1.1205e+00, -1.7833e+00,\n",
      "        -1.1350e+00,  3.8417e-01,  1.7504e+00, -7.6636e-02,  1.4452e+00,\n",
      "        -6.0269e-01, -2.5969e-01, -3.1866e-01,  1.1512e+00, -2.8843e-01,\n",
      "        -3.3350e-01,  1.1309e+00, -3.5047e-01, -3.6075e-01,  1.6250e-01,\n",
      "        -4.3681e-01, -3.7052e-01,  1.6351e-01, -4.8572e-01, -4.2935e-01,\n",
      "         1.3153e-01, -4.4013e-01, -3.4818e-01,  8.1006e-02,  4.6134e-01,\n",
      "        -1.7441e-01, -2.2826e-01,  2.8530e-01, -2.2858e-01,  1.1401e-01,\n",
      "         6.4136e-01,  1.1878e-02, -1.7134e-01,  9.6219e-01,  6.1134e-02,\n",
      "        -5.1080e-01,  1.1349e+00, -1.2643e-01, -5.3756e-01,  8.9963e-01,\n",
      "         5.1361e-01, -1.2510e+00,  8.2199e-01, -2.2060e-02, -1.1444e+00,\n",
      "         1.0457e+00, -2.1469e-01, -9.4436e-01,  6.7131e-01,  8.5110e-02,\n",
      "        -7.2982e-01,  4.1160e-01,  1.1652e-01,  3.4797e-01,  8.4727e-01,\n",
      "         1.9421e-01, -1.1119e+00,  1.6749e+00,  6.2694e-02, -1.7020e+00,\n",
      "         1.5978e+00,  1.6563e-01, -8.2167e-01, -9.7340e-01, -6.2888e-01,\n",
      "         8.8812e-01, -9.4038e-01, -6.4310e-01,  9.4933e-01, -9.0346e-01,\n",
      "        -6.8891e-01,  8.2443e-01, -9.3481e-01, -5.6279e-01,  8.5070e-01,\n",
      "         4.5033e-01,  2.2045e-01, -4.7129e-02,  5.6090e-01,  4.8699e-03,\n",
      "        -1.8345e-01,  6.2459e-01,  1.3602e-01,  7.6443e-02,  9.0680e-01,\n",
      "         5.4076e-02, -2.0138e-01,  7.3764e-01,  1.8177e-01, -2.6781e-02,\n",
      "         7.5633e-01,  3.2937e-01, -5.7069e-01,  1.1450e+00, -3.4931e-01,\n",
      "        -5.3505e-02,  7.4297e-01, -4.5462e-01,  3.3755e-01,  4.0126e-01,\n",
      "        -4.5187e-01,  9.8893e-01,  4.6756e-01, -3.2599e-01,  2.8113e-01,\n",
      "         9.5492e-01, -5.4366e-01, -1.7603e-01,  1.1082e+00, -3.8802e-01,\n",
      "         8.5047e-03,  8.4310e-01,  5.3148e-02, -6.4942e-01,  8.3081e-01,\n",
      "        -3.5317e-01, -6.0235e-01, -1.3127e-01, -3.2288e-01,  1.7132e+00,\n",
      "         1.2076e+00,  1.1571e-01, -1.5544e+00,  1.4102e+00, -4.8493e-02,\n",
      "        -1.4804e+00,  6.4658e-01, -3.3464e-01, -1.3712e+00,  1.4330e+00,\n",
      "         9.3763e-02, -1.8719e+00, -1.4634e+00,  8.1092e-01,  6.6873e-01,\n",
      "        -1.4625e+00,  8.3839e-01,  5.1207e-01, -1.3202e+00,  7.7709e-01,\n",
      "         4.1915e-01, -1.2908e+00,  7.2701e-01,  5.4645e-01, -1.2683e+00,\n",
      "         6.6130e-01,  5.9916e-01, -1.2757e+00,  8.9245e-01,  3.4685e-01,\n",
      "         1.0891e-01,  1.2147e-01,  2.1838e-01, -5.7215e-02,  4.3406e-02,\n",
      "         2.9680e-01,  1.1108e+00,  2.4463e-01, -7.9693e-01,  1.0346e+00,\n",
      "         2.3847e-01, -1.3314e+00, -2.4965e-01,  1.2537e+00, -3.7359e-01,\n",
      "         3.3903e-02,  1.0277e+00, -5.2134e-01, -3.1567e-01,  9.5221e-01,\n",
      "        -3.1103e-01, -2.4824e-03,  1.0720e+00, -2.9370e-01, -6.6110e-03,\n",
      "         1.0708e+00, -1.4006e-01, -1.1785e-01,  1.1546e+00, -1.9069e-01,\n",
      "        -8.7057e-02,  1.1560e+00, -2.3195e-01, -1.6511e-01,  1.1370e+00,\n",
      "        -5.7106e-02, -7.3455e-01,  4.3944e-01,  6.1899e-01, -6.6699e-01,\n",
      "         4.6799e-01,  6.3751e-01, -7.1806e-01,  3.6341e-01,  6.5097e-01,\n",
      "         1.0746e+00, -1.3347e-01, -1.1873e+00,  1.1898e+00, -3.2141e-01,\n",
      "        -8.6002e-01,  1.2552e+00, -3.0139e-01, -8.9323e-01,  1.1647e+00,\n",
      "        -1.1129e-01, -6.5490e-01,  7.4924e-01, -2.6024e-02, -5.6521e-01,\n",
      "        -1.3820e+00,  1.9053e-01,  1.0478e+00, -1.3920e+00,  2.3522e-01,\n",
      "         1.0315e+00, -1.3376e+00,  2.2568e-01,  9.8877e-01, -1.3136e+00,\n",
      "         2.8300e-01,  8.9332e-01, -1.3175e+00,  2.1513e-01,  1.0226e+00,\n",
      "        -1.2452e+00,  7.2010e-01,  1.9886e+00,  6.8943e-01,  1.0250e+00,\n",
      "        -1.7069e+00, -3.4858e-02,  9.4331e-01, -5.9356e-01, -4.7660e-01,\n",
      "         8.3436e-01, -8.9897e-02, -5.8605e-01,  9.4886e-01, -1.5137e-01,\n",
      "         5.8343e-02,  6.9532e-01, -5.3125e-01,  3.3366e-01,  8.3508e-01,\n",
      "        -8.5041e-01,  4.9636e-01,  1.2965e+00, -1.6133e+00, -2.0727e-01,\n",
      "         1.4797e+00, -8.0450e-01,  2.0929e-01,  1.5080e+00, -1.4284e+00,\n",
      "         1.4602e-01,  1.1496e+00, -8.3660e-01,  5.1477e-01,  1.1028e+00,\n",
      "        -1.3840e+00, -1.3317e+00,  8.1029e-01,  1.8085e+00, -1.3421e+00,\n",
      "         7.9174e-01,  1.7886e+00, -1.3210e+00,  7.2867e-01,  1.8144e+00,\n",
      "        -1.3287e+00,  7.1935e-01,  1.8352e+00, -1.2932e+00,  7.4602e-01,\n",
      "         1.8271e+00, -1.2916e+00,  7.4458e-01,  1.7775e+00, -1.2516e+00,\n",
      "         7.4624e-01,  1.7078e+00, -3.7631e-01,  3.5928e-01,  1.3783e+00,\n",
      "        -3.6104e-01,  3.6402e-01,  1.3276e+00, -2.6471e-01,  3.6942e-01,\n",
      "         1.1531e+00, -3.5372e-01, -4.5256e-01, -1.4700e-01, -4.5872e-01,\n",
      "        -3.1729e-01,  2.4054e-02, -4.0917e-01, -2.7227e-01, -9.1415e-02,\n",
      "        -1.0224e-01, -2.4888e-01, -4.5995e-02, -8.4608e-01,  6.0616e-01,\n",
      "        -8.6857e-01, -1.0901e+00,  5.6246e-01, -2.6497e-01,  6.8567e-01,\n",
      "         4.9759e-01, -1.1894e+00,  1.3573e+00, -6.7154e-02, -5.7149e-01,\n",
      "         7.4524e-01,  5.0313e-01, -7.0437e-01,  3.7273e-01,  3.7809e-01,\n",
      "        -8.6219e-01, -9.6629e-01, -5.8146e-01,  8.2739e-01], device='cuda:0')\n",
      "global_step: 40\n",
      "Epoch 0: 100%|██████████| 40/40 [34:26<00:00, 51.67s/it, v_num=4, val_loss=0.685, val_acc=0.750, train_loss=0.852, train_acc=0.619]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 40/40 [34:28<00:00, 51.71s/it, v_num=4, val_loss=0.685, val_acc=0.750, train_loss=0.852, train_acc=0.619]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "        max_epochs=args['max_epochs'],\n",
    "        callbacks=[TQDMProgressBar(refresh_rate=args['batch_size'])],\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"auto\",\n",
    "        devices=1 if torch.cuda.is_available() else None,\n",
    "        log_every_n_steps=40\n",
    "    )\n",
    "classification_module = VideoClassificationLightningModule(model, args)\n",
    "data_module = FlyDataModule(args)\n",
    "trainer.fit(classification_module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"args\": {'train_data_path': '/cta/users/mpekey/FlyVideo/FlyTrainingData/Train', 'val_data_path': '/cta/users/mpekey/FlyVideo/FlyTrainingData/Validation', 'lr': 0.001, 'max_epochs': 1, 'batch_size': 16, 'video_path_prefix': '', 'video_min_short_side_scale': 256, 'video_max_short_side_scale': 320, 'clip_duration': 4.266666666666667, 'crop_size': (224, 224), 'num_frames_to_sample': 8, 'video_means': [0.485, 0.456, 0.406], 'video_stds': [0.229, 0.224, 0.225], 'sample_rate': 16, 'fps': 30, 'num_frames': 8}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_module.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_kernel",
   "language": "python",
   "name": "lightning_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
