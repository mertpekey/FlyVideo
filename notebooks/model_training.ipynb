{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.progress import TQDMProgressBar\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit Dataset\n",
    "\n",
    " To ensure a constant number of samples are retrieved from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fly Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlyDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        super().__init__()\n",
    "\n",
    "    def _make_transforms(self, mode: str):\n",
    "        return Compose([self._video_transform(mode)])\n",
    "\n",
    "    def _video_transform(self, mode: str):\n",
    "        return ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(self.args[\"num_frames_to_sample\"]),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(self.args[\"video_means\"], self.args[\"video_stds\"]),\n",
    "                ]\n",
    "                + (\n",
    "                    [\n",
    "                        RandomShortSideScale(\n",
    "                            min_size=self.args[\"video_min_short_side_scale\"],\n",
    "                            max_size=self.args[\"video_max_short_side_scale\"],\n",
    "                        ),\n",
    "                        RandomCrop(self.args[\"crop_size\"]),\n",
    "                    ]\n",
    "                    if mode == \"train\"\n",
    "                    else [\n",
    "                        ShortSideScale(self.args[\"video_min_short_side_scale\"]),\n",
    "                        CenterCrop(self.args[\"crop_size\"]),\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        train_transform = self._make_transforms(mode=\"train\")\n",
    "\n",
    "        self.train_dataset = LimitDataset(\n",
    "            pytorchvideo.data.labeled_video_dataset(\n",
    "                data_path=self.args[\"train_data_path\"],\n",
    "                clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', self.args[\"clip_duration\"]), # Experiment olarak random da denenebilir\n",
    "                transform=train_transform,\n",
    "                video_path_prefix=self.args[\"video_path_prefix\"], # could be '' I think\n",
    "                decode_audio=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        val_transform = self._make_transforms(mode=\"val\")\n",
    "\n",
    "        self.val_dataset = LimitDataset(\n",
    "            pytorchvideo.data.labeled_video_dataset(\n",
    "                data_path=self.args[\"val_data_path\"],\n",
    "                clip_sampler=pytorchvideo.data.make_clip_sampler('uniform', self.args[\"clip_duration\"]), # Experiment olarak random da denenebilir\n",
    "                transform=val_transform,\n",
    "                video_path_prefix=self.args[\"video_path_prefix\"], # could be '' I think\n",
    "                decode_audio=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.args[\"batch_size\"]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassificationLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, args):\n",
    "\n",
    "        self.args = args\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, stage='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, stage='val')\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx, stage='train'):\n",
    "        X, y = batch['video'], batch['label']\n",
    "\n",
    "        output = self.model(X.permute(0, 2, 1, 3, 4)) # (8, 3, 16, 224, 224) -> (8, 16, 3, 224, 224)\n",
    "\n",
    "        loss = F.cross_entropy(output.logits, y)\n",
    "        acc = torchmetrics.functional.accuracy(output.logits, y, task=\"multiclass\", num_classes=3)\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\n",
    "            f\"{stage}_acc\", acc, on_step=True, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        if stage == 'train':\n",
    "            return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.args[\"lr\"],\n",
    "            weight_decay=self.args[\"weight_decay\"],\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, self.args[\"max_epochs\"], last_epoch=-1\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor_config(model, image_processor, sample_rate=8, fps=30):\n",
    "\n",
    "    mean = image_processor.image_mean\n",
    "    std = image_processor.image_std\n",
    "\n",
    "    if \"shortest_edge\" in image_processor.size:\n",
    "        height = width = image_processor.size[\"shortest_edge\"]\n",
    "    else:\n",
    "        height = image_processor.size[\"height\"]\n",
    "        width = image_processor.size[\"width\"]\n",
    "\n",
    "    crop_size = (height, width)\n",
    "\n",
    "    num_frames_to_sample = model.config.num_frames # 16 for VideoMAE\n",
    "    clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "    print('Clip Duration:', clip_duration, 'seconds')\n",
    "\n",
    "    return {\n",
    "        \"image_mean\" : mean,\n",
    "        \"image_std\" : std,\n",
    "        \"crop_size\" : crop_size,\n",
    "        \"num_frames_to_sample\" : num_frames_to_sample,\n",
    "        \"clip_duration\": clip_duration,\n",
    "        \"sample_rate\" : sample_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: ['decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'mask_token', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.q_bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.q_bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.v_bias', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.q_bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.head.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'encoder_to_decoder.weight', 'decoder.norm.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.q_bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.norm.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.v_bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.head.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.v_bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.v_bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.value.weight']\n",
      "- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip Duration: 4.266666666666667 seconds\n"
     ]
    }
   ],
   "source": [
    "# PATH INFO\n",
    "PROJ_DIR = '/Users/mpekey/Desktop/FlyVideo'\n",
    "TRAIN_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Train')\n",
    "VAL_DATA_PATH = os.path.join(PROJ_DIR, 'FlyTrainingData', 'Validation')\n",
    "\n",
    "# MODEL INFO\n",
    "MODEL_CHECKPOINT = \"MCG-NJU/videomae-base\"\n",
    "\n",
    "# DATASET INFO\n",
    "class_labels = ['Feeding', 'Grooming', 'Pumping']\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "    num_frames = 16 # Default is 16\n",
    ")\n",
    "\n",
    "model_args = create_preprocessor_config(model, image_processor, sample_rate=8, fps=30)\n",
    "\n",
    "args = {\n",
    "    # Data\n",
    "    \"train_data_path\" : TRAIN_DATA_PATH,\n",
    "    \"val_data_path\" : VAL_DATA_PATH,\n",
    "    \"lr\" : 0.1,\n",
    "    \"weight_decay\" : 1e-4,\n",
    "    \"max_epochs\" : 1,\n",
    "    \"batch_size\" : 8,\n",
    "    \"video_path_prefix\" : '',\n",
    "    \"video_min_short_side_scale\" : 256,\n",
    "    \"video_max_short_side_scale\" : 320,\n",
    "    \"clip_duration\" : model_args[\"clip_duration\"],\n",
    "    \"crop_size\" : model_args[\"crop_size\"],\n",
    "    \"num_frames_to_sample\": model_args[\"num_frames_to_sample\"],\n",
    "    \"video_means\" : model_args[\"image_mean\"],\n",
    "    \"video_stds\" : model_args[\"image_std\"]\n",
    "}\n",
    "\n",
    "# Freeze the model\n",
    "for param in model.videomae.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type                           | Params\n",
      "---------------------------------------------------------\n",
      "0 | model | VideoMAEForVideoClassification | 86.2 M\n",
      "---------------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "86.2 M    Non-trainable params\n",
      "86.2 M    Total params\n",
      "344.918   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 80/80 [1:49:03<00:00, 81.80s/it, v_num=0, train_loss_step=3.65e-5, train_acc_step=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 80/80 [2:30:40<00:00, 113.01s/it, v_num=0, train_loss_step=3.65e-5, train_acc_step=1.000, val_loss_step=46.60, val_acc_step=0.000, val_loss_epoch=36.70, val_acc_epoch=0.260, train_loss_epoch=15.30, train_acc_epoch=0.454]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpekey/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 80/80 [2:30:41<00:00, 113.02s/it, v_num=0, train_loss_step=3.65e-5, train_acc_step=1.000, val_loss_step=46.60, val_acc_step=0.000, val_loss_epoch=36.70, val_acc_epoch=0.260, train_loss_epoch=15.30, train_acc_epoch=0.454]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=args[\"max_epochs\"],\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=8)],\n",
    "    accelerator=\"auto\",\n",
    "    #devices=1 if torch.cuda.is_available() else None,\n",
    ")\n",
    "classification_module = VideoClassificationLightningModule(model, args)\n",
    "data_module = FlyDataModule(args)\n",
    "trainer.fit(classification_module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
